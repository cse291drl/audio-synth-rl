{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f588b90-45c6-4d60-851b-ac72fb956404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2009c7de-f2c8-4fdf-8f72-6b224a23f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from data_modules import data_modules as dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06bf7687-bf57-466f-8a17-9c98587092a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, feature_extractor_network: nn.Module, feature_dims : int, parameter_vector_dims : int):\n",
    "        super().__init__()\n",
    "        self.feature_extractor_network = feature_extractor_network\n",
    "        self.actor_head = nn.Sequential(\n",
    "            # Note:\n",
    "            # input num dims = 2 * feature_dims + parameter_vector_dims\n",
    "            # because actor head will get a concatenation of the following:\n",
    "            #    - feature vector of target spectrogram\n",
    "            #    - feature vector of prior spectrogram generated from the prior predicted parameter vector\n",
    "            #    - feature vector of prior predicted parameter vector\n",
    "            nn.Linear(2 * feature_dims + parameter_vector_dims, parameter_vector_dims*2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(parameter_vector_dims*2, parameter_vector_dims),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(parameter_vector_dims, parameter_vector_dims),\n",
    "\n",
    "            # This final sigmoid activation ensures each param output is in the [0,1] range\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, target_spectrogram, prior_spectrogram, prior_parameter_vector):\n",
    "        # Extract spectrogram features\n",
    "        target_spectrogram_feats = self.feature_extractor_network(target_spectrogram)\n",
    "        prior_spectrogram_feats = self.feature_extractor_network(prior_spectrogram)\n",
    "        state_vec = torch.hstack((target_spectrogram_feats, prior_spectrogram_feats, prior_parameter_vector))\n",
    "        \n",
    "        # Get action from actor network\n",
    "        pi = self.actor_head(state_vec)\n",
    "        \n",
    "        return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c85bbd2e-47e5-409a-9835-5166018079f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, feature_extractor_network: nn.Module, feature_dims : int, parameter_vector_dims : int):\n",
    "        super().__init__()\n",
    "        self.feature_extractor_network = feature_extractor_network\n",
    "                \n",
    "        self.critic_head = nn.Sequential(\n",
    "            # See prior note for explanation behind number of input dims here\n",
    "            nn.Linear(2 * feature_dims + parameter_vector_dims, 256*2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256*2, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 1),\n",
    "            \n",
    "            # We use a final tanh for the critic head's output value, to squash it between 1 and -1\n",
    "            # nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, target_spectrogram, prior_spectrogram, prior_parameter_vector):\n",
    "        # Extract spectrogram features\n",
    "        # print(f\"Target {target_spectrogram.get_device()}\")\n",
    "        # print(f\"Prior {prior_spectrogram.get_device()}\")\n",
    "        target_spectrogram_feats = self.feature_extractor_network(target_spectrogram)\n",
    "        prior_spectrogram_feats = self.feature_extractor_network(prior_spectrogram)\n",
    "        state_vec = torch.hstack((target_spectrogram_feats, prior_spectrogram_feats, prior_parameter_vector))\n",
    "        \n",
    "        # Get predicted Q(s,a) value from critic network\n",
    "        value = self.critic_head(state_vec)\n",
    "        \n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee626dbb-e519-46b7-8957-c3dbcccc878f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['audio', 'params', 'spectrogram'])\n"
     ]
    }
   ],
   "source": [
    "# Dummy spectrogram\n",
    "# spectrogram_shape = (1, 4, 8)\n",
    "# parameter_vector_dims = 16 #np.prod(spectrogram_shape)\n",
    "feature_vector_dims = 1024\n",
    "\n",
    "# Actual spectrogram\n",
    "preset_data = dm.TargetSoundDataModule(\n",
    "    data_dir=os.path.join('data', 'preset_data'),\n",
    "    split_file='split_dict.json',\n",
    "    num_workers=3,\n",
    "    batch_size = 1,\n",
    "    shuffle=True,\n",
    "    return_params = True\n",
    ")\n",
    "preset_data.setup()\n",
    "t0 = next(iter(preset_data.train_dataloader()))\n",
    "print(t0.keys()) # Check that params are also returned\n",
    "# Set up stuff\n",
    "audiohandler = dm.AudioHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59156b1b-9e77-4573-ac42-119c8c8aa34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE - For dummy spectrogram\n",
    "\n",
    "# [parameter vector dims, spectrogram flattened num dims]\n",
    "# parameter_vector_to_spectrogram_matrix = ortho_group.rvs(dim=np.prod(spectrogram_shape)).astype(np.float32)\n",
    "# parameter_vector_to_spectrogram_matrix = torch.from_numpy(parameter_vector_to_spectrogram_matrix).to(device).requires_grad_(False) \n",
    "# parameter_vector_to_spectrogram_matrix = torch.randn(parameter_vector_dims, np.prod(spectrogram_shape), requires_grad=False, device=device)\n",
    "# parameter_vector_to_spectrogram_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c34c503-69b9-42a8-b65f-5f628af26593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE - For dummy spectrogram\n",
    "# def generate_spectrogram_from_parameter_vector(parameter_vector):\n",
    "#     # [N, spectrogram flattened num dims]\n",
    "#     spectrogram_flattened = parameter_vector @ parameter_vector_to_spectrogram_matrix\n",
    "#     spectrogram = torch.reshape(spectrogram_flattened, (-1,) + spectrogram_shape)\n",
    "#     return spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202764d7-713a-4834-9f49-b87027f0b5f8",
   "metadata": {},
   "source": [
    "## Demo training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bcf889a-4a89-4788-b56e-e0822ee371cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm.notebook  import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70fcd853-58b8-43f9-ac1b-b82af38376fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 155]), torch.Size([1, 257, 345]), 155)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IGNORE - For dummy spectrogram\n",
    "# ground_truth_params_vector = torch.randn(1, parameter_vector_dims, device=device)\n",
    "# ground_truth_spectrogram = generate_spectrogram_from_parameter_vector(ground_truth_params_vector)\n",
    "# ground_truth_params_vector.shape, ground_truth_spectrogram.shape\n",
    "\n",
    "ground_truth_params_vector = t0['params']\n",
    "ground_truth_spectrogram = t0['spectrogram']\n",
    "ground_truth_audio = t0['audio']\n",
    "parameter_vector_dims = np.prod(ground_truth_params_vector.shape)\n",
    "ground_truth_params_vector.shape, ground_truth_spectrogram.shape, parameter_vector_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5337ff1-d313-4607-9229-a214d8747456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These should be close if parameter_vector_to_spectrogram_matrix is orthonormal\n",
    "# ground_truth_params_vector, (ground_truth_spectrogram.reshape(1, -1) @ parameter_vector_to_spectrogram_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14274a0b-1a10-4029-bb6c-d1300d2d4be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE - For dummy spectrogram\n",
    "def compute_mse_between_spectrograms(spec1, spec2):\n",
    "    # output: [N,1]\n",
    "    return ((spec1 - spec2)**2).mean(axis=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10a75acb-639d-4d80-b4bc-6244a82fcdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE - For dummy spectrogram\n",
    "# def compute_reward(spec1, spec2):\n",
    "#     # output: [N,1]\n",
    "    \n",
    "#     # first, we compute mse loss and use that to construct a reward\n",
    "#     mse_loss = compute_mse_between_spectrograms(spec1, spec2)\n",
    "    \n",
    "#     # with the transformation below, higher mse loss -> lower reward, and lower mse loss -> higher reward\n",
    "#     reward = -mse_loss #1/mse_loss #-torch.log(mse_loss) #1/mse_loss\n",
    "    \n",
    "#     # finally, to keep reward within [-1,1] range, we use tanh\n",
    "#     # reward = torch.tanh(reward)\n",
    "    \n",
    "#     return reward\n",
    "\n",
    "# Actual spectrogram reward\n",
    "def compute_reward(spec1, spec2):\n",
    "    # output: [N,1]\n",
    "    \n",
    "    # first, we compute mse loss and use that to construct a reward\n",
    "    mse_loss = compute_mse_between_spectrograms(spec1, spec2)\n",
    "    \n",
    "    # with the transformation below, higher mse loss -> lower reward, and lower mse loss -> higher reward\n",
    "    reward = -mse_loss #1/mse_loss #-torch.log(mse_loss) #1/mse_loss\n",
    "    \n",
    "    # finally, to keep reward within [-1,1] range, we use tanh\n",
    "    # reward = torch.tanh(reward)\n",
    "    \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "662c1bbb-1917-49fa-946b-5136ec74ac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    https://github.com/yc930401/Actor-Critic-pytorch/blob/5d359bc93839357255b591c0a87fc42542854eb8/Actor-Critic.py#L50\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    \n",
    "    R = rewards[-1]\n",
    "\n",
    "    for idx, step in enumerate(reversed(range(len(rewards)))):\n",
    "        if idx == 0:\n",
    "            R = rewards[step]\n",
    "        else:\n",
    "            R = rewards[step] + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3830e93b-237a-4f66-a79b-c2f1d24de2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummySpectrogramFeatureExtractorNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a dummy placeholder spectrogram feature extractor, that will convert any input spectrogram into a output feature vector.\n",
    "    The pre-trained VAE, for example, would ideally replace this.\n",
    "\n",
    "    Args:\n",
    "        spectrogram_shape (tuple of ints): The shape of the input spectrogram\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, spectrogram_shape, feature_dims : int):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(np.prod(spectrogram_shape), feature_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dims, feature_dims)\n",
    "        )\n",
    "    \n",
    "    def forward(self, spectrogram):\n",
    "        # flatten spectrogram into [N, flattened dims]\n",
    "        spectrogram_flattened = torch.flatten(spectrogram, start_dim=0)\n",
    "        return self.model(spectrogram_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "875976b5-d21a-4dad-9773-ffd37dd72207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_next_state(predicted_parameter_vector):\n",
    "#     next_spectrogram = generate_spectrogram_from_parameter_vector(predicted_parameter_vector)\n",
    "#     return [ground_truth_spectrogram, next_spectrogram, predicted_parameter_vector]\n",
    "\n",
    "def get_next_state(ground_truth_spectrogram, predicted_parameter_vector, audiohandler, device):\n",
    "    audio, next_spectrogram = audiohandler.generateAudio(predicted_parameter_vector)\n",
    "    return [ground_truth_spectrogram.to(device), next_spectrogram.to(device), predicted_parameter_vector.to(device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab700e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dummy feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fc6e852-291b-4ee1-a9e5-5d5935ded341",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTOR_LR = 1e-5\n",
    "CRITIC_LR = 1e-4\n",
    "\n",
    "num_episodes = 1_000\n",
    "num_steps_per_episode = 25\n",
    "gamma = 0.99\n",
    "\n",
    "GRADIENT_CLIPPING = 0.01\n",
    "\n",
    "step_optimizer_every_n_episode_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db1354d8-41a1-4f35-b676-889a534539c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_network = DummySpectrogramFeatureExtractorNetwork(spectrogram_shape=ground_truth_spectrogram.shape, feature_dims=feature_vector_dims)\n",
    "_ = feature_extractor_network.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1687b14-888f-4b5b-b6df-35905466ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_model = ActorNetwork(feature_extractor_network=feature_extractor_network, feature_dims=feature_vector_dims, parameter_vector_dims=parameter_vector_dims).to(device)\n",
    "critic_model = CriticNetwork(feature_extractor_network=feature_extractor_network, feature_dims=feature_vector_dims, parameter_vector_dims=parameter_vector_dims).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f13eaaec-7345-4ea5-b434-ba6c1b38ad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optimizer = Adam(actor_model.parameters(), lr=ACTOR_LR)\n",
    "critic_optimizer = Adam(critic_model.parameters(), lr=CRITIC_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1dd6291f-2132-417d-8ba4-af5790b89323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3f3098d6914a3ea7841f924d826bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829c7322911441e5aa95f9e6b9a5fff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: total reward: nan, return: nan\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6cb0d6a960f4c19969279585a3deb99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18688/3543784438.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactor_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcritic_loss_weight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m# print(loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep_idx\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstep_optimizer_every_n_episode_steps\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\synth\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\synth\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episode_pbar = tqdm(range(num_episodes))\n",
    "for episode_idx in episode_pbar: #range(num_episodes):\n",
    "    # Set initial state for episode\n",
    "    prior_predicted_parameter_vector = torch.rand(parameter_vector_dims, device=device)\n",
    "    audio, prior_predicted_spectrogram = audiohandler.generateAudio(prior_predicted_parameter_vector.cpu().detach())\n",
    "    \n",
    "    state = (ground_truth_spectrogram.to(device), prior_predicted_spectrogram.to(device), prior_predicted_parameter_vector.to(device))\n",
    "    \n",
    "    all_rewards = []\n",
    "    \n",
    "    episode_step_pbar = tqdm(range(num_steps_per_episode))\n",
    "    for step_idx in episode_step_pbar:\n",
    "        actor_optimizer.zero_grad()\n",
    "        critic_optimizer.zero_grad()\n",
    "        \n",
    "        # Get predicted action and value for current state\n",
    "        predicted_parameter_vector = actor_model(*state)\n",
    "        value_of_current_state = critic_model(*state)\n",
    "        # print(f\"predicted_parameter_vector {predicted_parameter_vector}\")\n",
    "        # print(f\"value_of_current_state {value_of_current_state}\")\n",
    "        # Perform action to get next state and reward\n",
    "        next_state = get_next_state(ground_truth_spectrogram.cpu(), predicted_parameter_vector.cpu().detach(), audiohandler, device=device)\n",
    "        reward = compute_reward(ground_truth_spectrogram.cpu().squeeze().detach().numpy(), next_state[1].cpu().squeeze().detach().numpy() )\n",
    "        \n",
    "        value_next_state = critic_model(*next_state)\n",
    "        # print(value_next_state.shape)\n",
    "        #TD Target: r + gamma * V(next_state)\n",
    "        td_target = reward + gamma * value_next_state\n",
    "        \n",
    "        # TODO: NaN loss, bugged\n",
    "        # print(f\"value_next_state {value_next_state}\")\n",
    "        delta = td_target - value_of_current_state\n",
    "        # print(f\"reward {reward}\")\n",
    "        actor_loss = delta * -torch.log(predicted_parameter_vector).mean()\n",
    "        critic_loss = delta ** 2\n",
    "        # print(f\"actor loss {actor_loss}\")\n",
    "        # print(f\"critic loss {critic_loss}\")\n",
    "        # We're weighting critic loss by 0.5\n",
    "        critic_loss_weight = 0.5\n",
    "        loss = actor_loss + critic_loss_weight * critic_loss\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        \n",
    "        if step_idx % step_optimizer_every_n_episode_steps == 0:\n",
    "            if GRADIENT_CLIPPING is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(actor_model.parameters(), GRADIENT_CLIPPING)\n",
    "                torch.nn.utils.clip_grad_norm_(critic_model.parameters(), GRADIENT_CLIPPING)\n",
    "\n",
    "            actor_optimizer.step()\n",
    "            critic_optimizer.step()\n",
    "        \n",
    "        # Now that we've learned from this step, update current state variable\n",
    "        next_state[1] = next_state[1].detach()\n",
    "        next_state[2] = next_state[2].detach()\n",
    "        state = next_state\n",
    "        \n",
    "        all_rewards.append(reward.item())\n",
    "        # print(predicted_parameter_vector)\n",
    "        # print(ground_truth_params_vector)\n",
    "        with torch.no_grad():\n",
    "            params_mse = ((predicted_parameter_vector.to(device) - ground_truth_params_vector.to(device))**2).mean().item()\n",
    "        episode_step_pbar.set_postfix({\"loss\" : loss.item(), \"reward\" : reward.item(), \"params_mse\" : params_mse})\n",
    "        # break\n",
    "    # break\n",
    "    print (f\"Episode {episode_idx + 1}: total reward: {sum(all_rewards)}, return: {compute_returns(all_rewards, gamma)[0]}\")\n",
    "    episode_pbar.set_postfix({\"total reward\" : sum(all_rewards), \"return\" : compute_returns(all_rewards, gamma)[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef21ee20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6aadfb841290bcfc72af95a2a07db7a853e47c0d88647a3676c8f524c44e33bf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('drl_project': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
