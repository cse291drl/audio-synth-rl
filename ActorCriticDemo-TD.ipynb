{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f588b90-45c6-4d60-851b-ac72fb956404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2009c7de-f2c8-4fdf-8f72-6b224a23f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from data_modules import data_modules as dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06bf7687-bf57-466f-8a17-9c98587092a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, feature_extractor_network: nn.Module, feature_dims : int, parameter_vector_dims : int):\n",
    "        super().__init__()\n",
    "        self.feature_extractor_network = feature_extractor_network\n",
    "        self.actor_head = nn.Sequential(\n",
    "            # Note:\n",
    "            # input num dims = 2 * feature_dims + parameter_vector_dims\n",
    "            # because actor head will get a concatenation of the following:\n",
    "            #    - feature vector of target spectrogram\n",
    "            #    - feature vector of prior spectrogram generated from the prior predicted parameter vector\n",
    "            #    - feature vector of prior predicted parameter vector\n",
    "            nn.Linear(2 * feature_dims + parameter_vector_dims, parameter_vector_dims*2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(parameter_vector_dims*2, parameter_vector_dims),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(parameter_vector_dims, parameter_vector_dims),\n",
    "\n",
    "            # This final sigmoid activation ensures each param output is in the [0,1] range\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, target_spectrogram, prior_spectrogram, prior_parameter_vector):\n",
    "        # Extract spectrogram features\n",
    "        target_spectrogram_feats = self.feature_extractor_network(target_spectrogram)\n",
    "        prior_spectrogram_feats = self.feature_extractor_network(prior_spectrogram)\n",
    "        state_vec = torch.hstack((target_spectrogram_feats, prior_spectrogram_feats, prior_parameter_vector))\n",
    "        \n",
    "        # Get action from actor network\n",
    "        pi = self.actor_head(state_vec)\n",
    "        \n",
    "        return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c85bbd2e-47e5-409a-9835-5166018079f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, feature_extractor_network: nn.Module, feature_dims : int, parameter_vector_dims : int):\n",
    "        super().__init__()\n",
    "        self.feature_extractor_network = feature_extractor_network\n",
    "                \n",
    "        self.critic_head = nn.Sequential(\n",
    "            # See prior note for explanation behind number of input dims here\n",
    "            nn.Linear(2 * feature_dims + parameter_vector_dims, 256*2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256*2, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 1),\n",
    "            \n",
    "            # We use a final tanh for the critic head's output value, to squash it between 1 and -1\n",
    "            # nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, target_spectrogram, prior_spectrogram, prior_parameter_vector):\n",
    "        # Extract spectrogram features\n",
    "        # print(f\"Target {target_spectrogram.get_device()}\")\n",
    "        # print(f\"Prior {prior_spectrogram.get_device()}\")\n",
    "        target_spectrogram_feats = self.feature_extractor_network(target_spectrogram)\n",
    "        prior_spectrogram_feats = self.feature_extractor_network(prior_spectrogram)\n",
    "        state_vec = torch.hstack((target_spectrogram_feats, prior_spectrogram_feats, prior_parameter_vector))\n",
    "        \n",
    "        # Get predicted Q(s,a) value from critic network\n",
    "        value = self.critic_head(state_vec)\n",
    "        \n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee626dbb-e519-46b7-8957-c3dbcccc878f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['audio', 'params', 'spectrogram'])\n"
     ]
    }
   ],
   "source": [
    "# Dummy spectrogram\n",
    "# spectrogram_shape = (1, 4, 8)\n",
    "# parameter_vector_dims = 16 #np.prod(spectrogram_shape)\n",
    "feature_vector_dims = 1024\n",
    "\n",
    "# Actual spectrogram\n",
    "preset_data = dm.TargetSoundDataModule(\n",
    "    data_dir=os.path.join('data', 'preset_data'),\n",
    "    split_file='split_dict.json',\n",
    "    num_workers=3,\n",
    "    batch_size = 1,\n",
    "    shuffle=True,\n",
    "    return_params = True\n",
    ")\n",
    "preset_data.setup()\n",
    "t0 = next(iter(preset_data.train_dataloader()))\n",
    "print(t0.keys()) # Check that params are also returned\n",
    "# Set up stuff\n",
    "audiohandler = dm.AudioHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59156b1b-9e77-4573-ac42-119c8c8aa34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE - For dummy spectrogram\n",
    "\n",
    "# [parameter vector dims, spectrogram flattened num dims]\n",
    "# parameter_vector_to_spectrogram_matrix = ortho_group.rvs(dim=np.prod(spectrogram_shape)).astype(np.float32)\n",
    "# parameter_vector_to_spectrogram_matrix = torch.from_numpy(parameter_vector_to_spectrogram_matrix).to(device).requires_grad_(False) \n",
    "# parameter_vector_to_spectrogram_matrix = torch.randn(parameter_vector_dims, np.prod(spectrogram_shape), requires_grad=False, device=device)\n",
    "# parameter_vector_to_spectrogram_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c34c503-69b9-42a8-b65f-5f628af26593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE - For dummy spectrogram\n",
    "# def generate_spectrogram_from_parameter_vector(parameter_vector):\n",
    "#     # [N, spectrogram flattened num dims]\n",
    "#     spectrogram_flattened = parameter_vector @ parameter_vector_to_spectrogram_matrix\n",
    "#     spectrogram = torch.reshape(spectrogram_flattened, (-1,) + spectrogram_shape)\n",
    "#     return spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202764d7-713a-4834-9f49-b87027f0b5f8",
   "metadata": {},
   "source": [
    "## Demo training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bcf889a-4a89-4788-b56e-e0822ee371cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70fcd853-58b8-43f9-ac1b-b82af38376fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 155]), torch.Size([1, 257, 345]), 155)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IGNORE - For dummy spectrogram\n",
    "# ground_truth_params_vector = torch.randn(1, parameter_vector_dims, device=device)\n",
    "# ground_truth_spectrogram = generate_spectrogram_from_parameter_vector(ground_truth_params_vector)\n",
    "# ground_truth_params_vector.shape, ground_truth_spectrogram.shape\n",
    "\n",
    "ground_truth_params_vector = t0['params']\n",
    "ground_truth_spectrogram = t0['spectrogram']\n",
    "ground_truth_audio = t0['audio']\n",
    "parameter_vector_dims = np.prod(ground_truth_params_vector.shape)\n",
    "ground_truth_params_vector.shape, ground_truth_spectrogram.shape, parameter_vector_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5337ff1-d313-4607-9229-a214d8747456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These should be close if parameter_vector_to_spectrogram_matrix is orthonormal\n",
    "# ground_truth_params_vector, (ground_truth_spectrogram.reshape(1, -1) @ parameter_vector_to_spectrogram_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14274a0b-1a10-4029-bb6c-d1300d2d4be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE - For dummy spectrogram\n",
    "def compute_mse_between_spectrograms(spec1, spec2):\n",
    "    # output: [N,1]\n",
    "    return ((spec1 - spec2)**2).mean(axis=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10a75acb-639d-4d80-b4bc-6244a82fcdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE - For dummy spectrogram\n",
    "# def compute_reward(spec1, spec2):\n",
    "#     # output: [N,1]\n",
    "    \n",
    "#     # first, we compute mse loss and use that to construct a reward\n",
    "#     mse_loss = compute_mse_between_spectrograms(spec1, spec2)\n",
    "    \n",
    "#     # with the transformation below, higher mse loss -> lower reward, and lower mse loss -> higher reward\n",
    "#     reward = -mse_loss #1/mse_loss #-torch.log(mse_loss) #1/mse_loss\n",
    "    \n",
    "#     # finally, to keep reward within [-1,1] range, we use tanh\n",
    "#     # reward = torch.tanh(reward)\n",
    "    \n",
    "#     return reward\n",
    "\n",
    "# Actual spectrogram reward\n",
    "def compute_reward(spec1, spec2):\n",
    "    # output: [N,1]\n",
    "    \n",
    "    # first, we compute mse loss and use that to construct a reward\n",
    "    mse_loss = compute_mse_between_spectrograms(spec1, spec2)\n",
    "    \n",
    "    # with the transformation below, higher mse loss -> lower reward, and lower mse loss -> higher reward\n",
    "    reward = -mse_loss #1/mse_loss #-torch.log(mse_loss) #1/mse_loss\n",
    "    \n",
    "    # finally, to keep reward within [-1,1] range, we use tanh\n",
    "    # reward = torch.tanh(reward)\n",
    "    \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "662c1bbb-1917-49fa-946b-5136ec74ac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    https://github.com/yc930401/Actor-Critic-pytorch/blob/5d359bc93839357255b591c0a87fc42542854eb8/Actor-Critic.py#L50\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    \n",
    "    R = rewards[-1]\n",
    "\n",
    "    for idx, step in enumerate(reversed(range(len(rewards)))):\n",
    "        if idx == 0:\n",
    "            R = rewards[step]\n",
    "        else:\n",
    "            R = rewards[step] + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3830e93b-237a-4f66-a79b-c2f1d24de2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummySpectrogramFeatureExtractorNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a dummy placeholder spectrogram feature extractor, that will convert any input spectrogram into a output feature vector.\n",
    "    The pre-trained VAE, for example, would ideally replace this.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, spectrogram_shape : tuple[int], feature_dims : int):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(np.prod(spectrogram_shape), feature_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dims, feature_dims)\n",
    "        )\n",
    "    \n",
    "    def forward(self, spectrogram):\n",
    "        # flatten spectrogram into [N, flattened dims]\n",
    "        spectrogram_flattened = torch.flatten(spectrogram, start_dim=0)\n",
    "        return self.model(spectrogram_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "875976b5-d21a-4dad-9773-ffd37dd72207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_next_state(predicted_parameter_vector):\n",
    "#     next_spectrogram = generate_spectrogram_from_parameter_vector(predicted_parameter_vector)\n",
    "#     return [ground_truth_spectrogram, next_spectrogram, predicted_parameter_vector]\n",
    "\n",
    "def get_next_state(ground_truth_spectrogram, predicted_parameter_vector, audiohandler, device):\n",
    "    audio, next_spectrogram = audiohandler.generateAudio(predicted_parameter_vector)\n",
    "    return [ground_truth_spectrogram.to(device), next_spectrogram.to(device), predicted_parameter_vector.to(device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab700e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dummy feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fc6e852-291b-4ee1-a9e5-5d5935ded341",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTOR_LR = 1e-9\n",
    "CRITIC_LR = 1e-8\n",
    "\n",
    "num_episodes = 1_000\n",
    "num_steps_per_episode = 1_00\n",
    "gamma = 0.99\n",
    "\n",
    "GRADIENT_CLIPPING = 0.01\n",
    "\n",
    "step_optimizer_every_n_episode_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db1354d8-41a1-4f35-b676-889a534539c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_network = DummySpectrogramFeatureExtractorNetwork(spectrogram_shape=ground_truth_spectrogram.shape, feature_dims=feature_vector_dims)\n",
    "_ = feature_extractor_network.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1687b14-888f-4b5b-b6df-35905466ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_model = ActorNetwork(feature_extractor_network=feature_extractor_network, feature_dims=feature_vector_dims, parameter_vector_dims=parameter_vector_dims).to(device)\n",
    "critic_model = CriticNetwork(feature_extractor_network=feature_extractor_network, feature_dims=feature_vector_dims, parameter_vector_dims=parameter_vector_dims).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f13eaaec-7345-4ea5-b434-ba6c1b38ad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optimizer = Adam(actor_model.parameters(), lr=ACTOR_LR)\n",
    "critic_optimizer = Adam(critic_model.parameters(), lr=CRITIC_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dd6291f-2132-417d-8ba4-af5790b89323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:01<?, ?it/s, loss=nan, reward=nan, params_mse=0.257]\n",
      "  0%|          | 0/1000 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_parameter_vector tensor([0.9503, 0.0879, 0.4526, 0.3286, 0.0894, 0.8229, 0.0935, 0.9525, 0.9960,\n",
      "        0.6931, 0.3791, 0.3311, 0.4486, 0.9582, 0.6784, 0.8286, 0.9353, 0.9366,\n",
      "        0.2177, 0.9026, 0.8711, 0.0658, 0.4316, 0.7656, 0.7646, 0.0814, 0.7100,\n",
      "        0.6752, 0.9239, 0.8133, 0.1706, 0.7685, 0.4450, 0.2497, 0.9927, 0.0041,\n",
      "        0.1987, 0.5134, 0.7998, 0.7241, 0.0659, 0.3664, 0.8363, 0.6985, 0.4658,\n",
      "        0.1252, 0.3599, 0.9907, 0.8322, 0.4194, 0.6232, 0.0367, 0.2499, 0.0732,\n",
      "        0.5058, 0.0015, 0.3950, 0.4055, 0.2156, 0.5820, 0.1767, 0.2538, 0.1763,\n",
      "        0.4342, 0.8024, 0.0425, 0.6188, 0.0759, 0.1770, 0.1257, 0.9688, 0.1353,\n",
      "        0.6636, 0.6895, 0.1389, 0.2710, 0.3197, 0.3680, 0.0430, 0.7992, 0.2915,\n",
      "        0.6067, 0.0291, 0.2426, 0.2113, 0.0395, 0.9691, 0.2020, 0.9298, 0.4228,\n",
      "        0.5649, 0.9000, 0.1025, 0.7228, 0.7514, 0.8766, 0.2069, 0.9645, 0.7439,\n",
      "        0.6479, 0.7932, 0.0511, 0.8922, 0.8471, 0.1157, 0.2993, 0.8362, 0.4723,\n",
      "        0.9980, 0.2918, 0.9068, 0.0101, 0.4060, 0.2540, 0.0987, 0.0922, 0.0118,\n",
      "        0.1236, 0.5373, 0.7643, 0.0954, 0.2065, 0.6484, 0.9385, 0.4758, 0.0750,\n",
      "        0.9225, 0.8694, 0.8982, 0.0990, 0.4054, 0.3430, 0.1920, 0.1701, 0.2616,\n",
      "        0.1416, 0.8920, 0.6426, 0.2630, 0.2708, 0.9552, 0.0127, 0.4805, 0.9783,\n",
      "        0.4239, 0.7121, 0.0375, 0.9557, 0.0171, 0.0923, 0.8964, 0.9795, 0.8238,\n",
      "        0.4665, 0.0619], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "value_of_current_state tensor([-0.8671], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "value_next_state tensor([nan], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "reward nan\n",
      "actor loss tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "critic loss tensor([nan], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "tensor([nan], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "episode_pbar = tqdm(range(num_episodes))\n",
    "for episode_idx in episode_pbar: #range(num_episodes):\n",
    "    # Set initial state for episode\n",
    "    prior_predicted_parameter_vector = torch.rand(parameter_vector_dims, device=device)\n",
    "    audio, prior_predicted_spectrogram = audiohandler.generateAudio(prior_predicted_parameter_vector.cpu().detach())\n",
    "    \n",
    "    state = (ground_truth_spectrogram.to(device), prior_predicted_spectrogram.to(device), prior_predicted_parameter_vector.to(device))\n",
    "    \n",
    "    all_rewards = []\n",
    "    \n",
    "    episode_step_pbar = tqdm(range(num_steps_per_episode))\n",
    "    for step_idx in episode_step_pbar:\n",
    "        actor_optimizer.zero_grad()\n",
    "        critic_optimizer.zero_grad()\n",
    "        \n",
    "        # Get predicted action and value for current state\n",
    "        predicted_parameter_vector = actor_model(*state)\n",
    "        value_of_current_state = critic_model(*state)\n",
    "        print(f\"predicted_parameter_vector {predicted_parameter_vector}\")\n",
    "        print(f\"value_of_current_state {value_of_current_state}\")\n",
    "        # Perform action to get next state and reward\n",
    "        next_state = get_next_state(ground_truth_spectrogram.cpu(), predicted_parameter_vector.cpu().detach(), audiohandler, device=device)\n",
    "        reward = compute_reward(ground_truth_spectrogram.cpu().squeeze().detach().numpy(), next_state[1].cpu().squeeze().detach().numpy() )\n",
    "        \n",
    "        value_next_state = critic_model(*next_state)\n",
    "        # print(value_next_state.shape)\n",
    "        #TD Target: r + gamma * V(next_state)\n",
    "        td_target = reward + gamma * value_next_state\n",
    "        \n",
    "        # TODO: NaN loss, bugged\n",
    "        print(f\"value_next_state {value_next_state}\")\n",
    "        delta = td_target - value_of_current_state\n",
    "        print(f\"reward {reward}\")\n",
    "        actor_loss = delta * -torch.log(predicted_parameter_vector).mean()\n",
    "        critic_loss = delta ** 2\n",
    "        print(f\"actor loss {actor_loss}\")\n",
    "        print(f\"critic loss {critic_loss}\")\n",
    "        # We're weighting critic loss by 0.5\n",
    "        critic_loss_weight = 0.5\n",
    "        loss = actor_loss + critic_loss_weight * critic_loss\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        \n",
    "        if step_idx % step_optimizer_every_n_episode_steps == 0:\n",
    "            if GRADIENT_CLIPPING is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(actor_model.parameters(), GRADIENT_CLIPPING)\n",
    "                torch.nn.utils.clip_grad_norm_(critic_model.parameters(), GRADIENT_CLIPPING)\n",
    "\n",
    "            actor_optimizer.step()\n",
    "            critic_optimizer.step()\n",
    "        \n",
    "        # Now that we've learned from this step, update current state variable\n",
    "        next_state[1] = next_state[1].detach()\n",
    "        next_state[2] = next_state[2].detach()\n",
    "        state = next_state\n",
    "        \n",
    "        all_rewards.append(reward.item())\n",
    "        # print(predicted_parameter_vector)\n",
    "        # print(ground_truth_params_vector)\n",
    "        with torch.no_grad():\n",
    "            params_mse = ((predicted_parameter_vector.to(device) - ground_truth_params_vector.to(device))**2).mean().item()\n",
    "        episode_step_pbar.set_postfix({\"loss\" : loss.item(), \"reward\" : reward.item(), \"params_mse\" : params_mse})\n",
    "        break\n",
    "    break\n",
    "    # print (f\"Episode {episode_idx + 1}: total reward: {sum(all_rewards)}, return: {compute_returns(all_rewards, gamma)[0]}\")\n",
    "    episode_pbar.set_postfix({\"total reward\" : sum(all_rewards), \"return\" : compute_returns(all_rewards, gamma)[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6aadfb841290bcfc72af95a2a07db7a853e47c0d88647a3676c8f524c44e33bf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('drl_project': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
