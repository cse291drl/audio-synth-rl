{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f588b90-45c6-4d60-851b-ac72fb956404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2009c7de-f2c8-4fdf-8f72-6b224a23f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from data_modules import data_modules as dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06bf7687-bf57-466f-8a17-9c98587092a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, feature_extractor_network: nn.Module, feature_dims : int, parameter_vector_dims : int):\n",
    "        super().__init__()\n",
    "        self.feature_extractor_network = feature_extractor_network\n",
    "        self.actor_head = nn.Sequential(\n",
    "            # Note:\n",
    "            # input num dims = 2 * feature_dims + parameter_vector_dims\n",
    "            # because actor head will get a concatenation of the following:\n",
    "            #    - feature vector of target spectrogram\n",
    "            #    - feature vector of prior spectrogram generated from the prior predicted parameter vector\n",
    "            #    - feature vector of prior predicted parameter vector\n",
    "            nn.Linear(2 * feature_dims + parameter_vector_dims, parameter_vector_dims*2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(parameter_vector_dims*2, parameter_vector_dims),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(parameter_vector_dims, parameter_vector_dims),\n",
    "\n",
    "            # This final sigmoid activation ensures each param output is in the [0,1] range\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, target_spectrogram, prior_spectrogram, prior_parameter_vector):\n",
    "        # Extract spectrogram features\n",
    "        target_spectrogram_feats = self.feature_extractor_network(target_spectrogram)\n",
    "        prior_spectrogram_feats = self.feature_extractor_network(prior_spectrogram)\n",
    "        state_vec = torch.hstack((target_spectrogram_feats, prior_spectrogram_feats, prior_parameter_vector))\n",
    "        \n",
    "        # Get action from actor network\n",
    "        pi = self.actor_head(state_vec)\n",
    "        \n",
    "        return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c85bbd2e-47e5-409a-9835-5166018079f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, feature_extractor_network: nn.Module, feature_dims : int, parameter_vector_dims : int):\n",
    "        super().__init__()\n",
    "        self.feature_extractor_network = feature_extractor_network\n",
    "                \n",
    "        self.critic_head = nn.Sequential(\n",
    "            # See prior note for explanation behind number of input dims here\n",
    "            nn.Linear(2 * feature_dims + parameter_vector_dims, 32*2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(32*2, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(32, 1),\n",
    "            \n",
    "            # We use a final tanh for the critic head's output value, to squash it between 1 and -1\n",
    "            # nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, target_spectrogram, prior_spectrogram, prior_parameter_vector):\n",
    "        # Extract spectrogram features\n",
    "        # print(f\"Target {target_spectrogram.get_device()}\")\n",
    "        # print(f\"Prior {prior_spectrogram.get_device()}\")\n",
    "        target_spectrogram_feats = self.feature_extractor_network(target_spectrogram)\n",
    "        prior_spectrogram_feats = self.feature_extractor_network(prior_spectrogram)\n",
    "        state_vec = torch.hstack((target_spectrogram_feats, prior_spectrogram_feats, prior_parameter_vector))\n",
    "        \n",
    "        # Get predicted Q(s,a) value from critic network\n",
    "        value = self.critic_head(state_vec)\n",
    "        \n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee626dbb-e519-46b7-8957-c3dbcccc878f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['audio', 'params', 'spectrogram'])\n"
     ]
    }
   ],
   "source": [
    "# Dummy spectrogram\n",
    "# spectrogram_shape = (1, 4, 8)\n",
    "# parameter_vector_dims = 16 #np.prod(spectrogram_shape)\n",
    "feature_vector_dims = 256\n",
    "\n",
    "# Actual spectrogram\n",
    "preset_data = dm.TargetSoundDataModule(\n",
    "    data_dir=os.path.join('data', 'preset_data'),\n",
    "    split_file='split_dict.json',\n",
    "    num_workers=3,\n",
    "    batch_size = 1,\n",
    "    shuffle=True,\n",
    "    return_params = True\n",
    ")\n",
    "preset_data.setup()\n",
    "t0 = next(iter(preset_data.train_dataloader()))\n",
    "print(t0.keys()) # Check that params are also returned\n",
    "# Set up stuff\n",
    "audiohandler = dm.AudioHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59156b1b-9e77-4573-ac42-119c8c8aa34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE - For dummy spectrogram\n",
    "\n",
    "# [parameter vector dims, spectrogram flattened num dims]\n",
    "# parameter_vector_to_spectrogram_matrix = ortho_group.rvs(dim=np.prod(spectrogram_shape)).astype(np.float32)\n",
    "# parameter_vector_to_spectrogram_matrix = torch.from_numpy(parameter_vector_to_spectrogram_matrix).to(device).requires_grad_(False) \n",
    "# parameter_vector_to_spectrogram_matrix = torch.randn(parameter_vector_dims, np.prod(spectrogram_shape), requires_grad=False, device=device)\n",
    "# parameter_vector_to_spectrogram_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c34c503-69b9-42a8-b65f-5f628af26593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE - For dummy spectrogram\n",
    "# def generate_spectrogram_from_parameter_vector(parameter_vector):\n",
    "#     # [N, spectrogram flattened num dims]\n",
    "#     spectrogram_flattened = parameter_vector @ parameter_vector_to_spectrogram_matrix\n",
    "#     spectrogram = torch.reshape(spectrogram_flattened, (-1,) + spectrogram_shape)\n",
    "#     return spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202764d7-713a-4834-9f49-b87027f0b5f8",
   "metadata": {},
   "source": [
    "## Demo training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bcf889a-4a89-4788-b56e-e0822ee371cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70fcd853-58b8-43f9-ac1b-b82af38376fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 155]), torch.Size([1, 257, 345]), 155)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IGNORE - For dummy spectrogram\n",
    "# ground_truth_params_vector = torch.randn(1, parameter_vector_dims, device=device)\n",
    "# ground_truth_spectrogram = generate_spectrogram_from_parameter_vector(ground_truth_params_vector)\n",
    "# ground_truth_params_vector.shape, ground_truth_spectrogram.shape\n",
    "\n",
    "ground_truth_params_vector = t0['params']\n",
    "ground_truth_spectrogram = t0['spectrogram']\n",
    "ground_truth_audio = t0['audio']\n",
    "parameter_vector_dims = np.prod(ground_truth_params_vector.shape)\n",
    "ground_truth_params_vector.shape, ground_truth_spectrogram.shape, parameter_vector_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5337ff1-d313-4607-9229-a214d8747456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These should be close if parameter_vector_to_spectrogram_matrix is orthonormal\n",
    "# ground_truth_params_vector, (ground_truth_spectrogram.reshape(1, -1) @ parameter_vector_to_spectrogram_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14274a0b-1a10-4029-bb6c-d1300d2d4be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE - For dummy spectrogram\n",
    "def compute_mse_between_spectrograms(spec1, spec2):\n",
    "    # output: [N,1]\n",
    "    return ((spec1 - spec2)**2).mean(axis=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10a75acb-639d-4d80-b4bc-6244a82fcdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE - For dummy spectrogram\n",
    "# def compute_reward(spec1, spec2):\n",
    "#     # output: [N,1]\n",
    "    \n",
    "#     # first, we compute mse loss and use that to construct a reward\n",
    "#     mse_loss = compute_mse_between_spectrograms(spec1, spec2)\n",
    "    \n",
    "#     # with the transformation below, higher mse loss -> lower reward, and lower mse loss -> higher reward\n",
    "#     reward = -mse_loss #1/mse_loss #-torch.log(mse_loss) #1/mse_loss\n",
    "    \n",
    "#     # finally, to keep reward within [-1,1] range, we use tanh\n",
    "#     # reward = torch.tanh(reward)\n",
    "    \n",
    "#     return reward\n",
    "\n",
    "# Actual spectrogram reward\n",
    "def compute_reward(spec1, spec2):\n",
    "    # output: [N,1]\n",
    "    \n",
    "    # first, we compute mse loss and use that to construct a reward\n",
    "    mse_loss = compute_mse_between_spectrograms(spec1, spec2)\n",
    "    \n",
    "    # with the transformation below, higher mse loss -> lower reward, and lower mse loss -> higher reward\n",
    "    reward = -mse_loss #1/mse_loss #-torch.log(mse_loss) #1/mse_loss\n",
    "    \n",
    "    # finally, to keep reward within [-1,1] range, we use tanh\n",
    "    # reward = torch.tanh(reward)\n",
    "    \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "662c1bbb-1917-49fa-946b-5136ec74ac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    https://github.com/yc930401/Actor-Critic-pytorch/blob/5d359bc93839357255b591c0a87fc42542854eb8/Actor-Critic.py#L50\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    \n",
    "    R = rewards[-1]\n",
    "\n",
    "    for idx, step in enumerate(reversed(range(len(rewards)))):\n",
    "        if idx == 0:\n",
    "            R = rewards[step]\n",
    "        else:\n",
    "            R = rewards[step] + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3830e93b-237a-4f66-a79b-c2f1d24de2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummySpectrogramFeatureExtractorNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a dummy placeholder spectrogram feature extractor, that will convert any input spectrogram into a output feature vector.\n",
    "    The pre-trained VAE, for example, would ideally replace this.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, spectrogram_shape : tuple[int], feature_dims : int):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(np.prod(spectrogram_shape), feature_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dims, feature_dims)\n",
    "        )\n",
    "    \n",
    "    def forward(self, spectrogram):\n",
    "        # flatten spectrogram into [N, flattened dims]\n",
    "        spectrogram_flattened = torch.flatten(spectrogram, start_dim=0)\n",
    "        return self.model(spectrogram_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "875976b5-d21a-4dad-9773-ffd37dd72207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_next_state(predicted_parameter_vector):\n",
    "#     next_spectrogram = generate_spectrogram_from_parameter_vector(predicted_parameter_vector)\n",
    "#     return [ground_truth_spectrogram, next_spectrogram, predicted_parameter_vector]\n",
    "\n",
    "def get_next_state(ground_truth_spectrogram, predicted_parameter_vector, audiohandler, device):\n",
    "    audio, next_spectrogram = audiohandler.generateAudio(predicted_parameter_vector)\n",
    "    return [ground_truth_spectrogram.to(device), next_spectrogram.to(device), predicted_parameter_vector.to(device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab700e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dummy feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fc6e852-291b-4ee1-a9e5-5d5935ded341",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTOR_LR = 1e-9\n",
    "CRITIC_LR = 1e-8\n",
    "\n",
    "num_episodes = 1_000\n",
    "num_steps_per_episode = 1_00\n",
    "gamma = 0.99\n",
    "\n",
    "GRADIENT_CLIPPING = 0.01\n",
    "\n",
    "step_optimizer_every_n_episode_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db1354d8-41a1-4f35-b676-889a534539c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_network = DummySpectrogramFeatureExtractorNetwork(spectrogram_shape=ground_truth_spectrogram.shape, feature_dims=feature_vector_dims)\n",
    "_ = feature_extractor_network.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1687b14-888f-4b5b-b6df-35905466ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_model = ActorNetwork(feature_extractor_network=feature_extractor_network, feature_dims=feature_vector_dims, parameter_vector_dims=parameter_vector_dims).to(device)\n",
    "critic_model = CriticNetwork(feature_extractor_network=feature_extractor_network, feature_dims=feature_vector_dims, parameter_vector_dims=parameter_vector_dims).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f13eaaec-7345-4ea5-b434-ba6c1b38ad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optimizer = Adam(actor_model.parameters(), lr=ACTOR_LR)\n",
    "critic_optimizer = Adam(critic_model.parameters(), lr=CRITIC_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dd6291f-2132-417d-8ba4-af5790b89323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:01<?, ?it/s, loss=nan, reward=nan, params_mse=0.279]\n",
      "  0%|          | 0/1000 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9079, 0.4722, 0.6367, 0.7020, 0.5979, 0.5093, 0.1061, 0.1802, 0.7952,\n",
      "        0.1374, 0.1815, 0.4979, 0.0528, 0.7544, 0.7948, 0.8472, 0.2139, 0.6088,\n",
      "        0.8711, 0.8837, 0.1678, 0.5946, 0.8419, 0.1097, 0.1325, 0.9127, 0.7809,\n",
      "        0.0676, 0.4376, 0.6097, 0.9484, 0.8268, 0.8228, 0.6888, 0.6368, 0.5003,\n",
      "        0.1300, 0.6796, 0.1490, 0.4400, 0.5753, 0.9209, 0.0234, 0.6747, 0.1786,\n",
      "        0.7496, 0.4552, 0.6188, 0.5069, 0.0088, 0.1736, 0.8079, 0.8917, 0.0484,\n",
      "        0.8950, 0.8404, 0.8019, 0.8510, 0.9893, 0.4366, 0.9498, 0.0271, 0.0192,\n",
      "        0.9693, 0.8285, 0.8832, 0.1002, 0.0351, 0.6253, 0.6979, 0.1381, 0.7927,\n",
      "        0.8758, 0.7552, 0.2098, 0.8078, 0.2487, 0.2619, 0.5521, 0.5295, 0.3122,\n",
      "        0.3446, 0.6198, 0.3494, 0.8419, 0.8612, 0.6750, 0.3113, 0.0673, 0.1447,\n",
      "        0.4208, 0.7905, 0.4107, 0.8755, 0.3782, 0.6332, 0.0273, 0.9319, 0.7365,\n",
      "        0.7524, 0.8097, 0.5070, 0.4110, 0.4930, 0.4627, 0.7300, 0.7322, 0.0846,\n",
      "        0.0316, 0.2416, 0.7468, 0.9828, 0.1702, 0.4242, 0.2502, 0.4942, 0.0425,\n",
      "        0.3707, 0.8296, 0.3599, 0.8253, 0.9388, 0.6017, 0.4484, 0.1059, 0.7218,\n",
      "        0.9581, 0.1039, 0.8860, 0.0625, 0.4908, 0.0276, 0.6288, 0.5087, 0.9063,\n",
      "        0.2456, 0.2459, 0.0194, 0.1798, 0.9950, 0.9774, 0.5936, 0.9917, 0.0875,\n",
      "        0.6049, 0.2412, 0.5790, 0.6782, 0.6302, 0.8684, 0.2705, 0.7853, 0.4169,\n",
      "        0.3004, 0.4059], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "tensor([1.3032], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([nan], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "nan\n",
      "actor loss tensor([nan], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "critic loss tensor([nan], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "tensor([nan], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "episode_pbar = tqdm(range(num_episodes))\n",
    "for episode_idx in episode_pbar: #range(num_episodes):\n",
    "    # Set initial state for episode\n",
    "    prior_predicted_parameter_vector = torch.rand(parameter_vector_dims, device=device)\n",
    "    audio, prior_predicted_spectrogram = audiohandler.generateAudio(prior_predicted_parameter_vector.cpu().detach())\n",
    "    \n",
    "    state = (ground_truth_spectrogram.to(device), prior_predicted_spectrogram.to(device), prior_predicted_parameter_vector.to(device))\n",
    "    \n",
    "    all_rewards = []\n",
    "    \n",
    "    episode_step_pbar = tqdm(range(num_steps_per_episode))\n",
    "    for step_idx in episode_step_pbar:\n",
    "        actor_optimizer.zero_grad()\n",
    "        critic_optimizer.zero_grad()\n",
    "        \n",
    "        # Get predicted action and value for current state\n",
    "        predicted_parameter_vector = actor_model(*state)\n",
    "        value_of_current_state = critic_model(*state)\n",
    "        print(predicted_parameter_vector)\n",
    "        print(value_of_current_state)\n",
    "        # Perform action to get next state and reward\n",
    "        next_state = get_next_state(ground_truth_spectrogram.cpu(), predicted_parameter_vector.cpu().detach(), audiohandler, device=device)\n",
    "        reward = compute_reward(ground_truth_spectrogram.cpu().squeeze().detach().numpy(), next_state[1].cpu().squeeze().detach().numpy() )\n",
    "        \n",
    "        value_next_state = critic_model(*next_state)\n",
    "        # print(value_next_state.shape)\n",
    "        #TD Target: r + gamma * V(next_state)\n",
    "        td_target = reward + gamma * value_next_state\n",
    "        \n",
    "        # TODO: NaN loss, bugged\n",
    "        print(value_next_state)\n",
    "        delta = td_target - value_of_current_state\n",
    "        print(reward)\n",
    "        actor_loss = delta * -torch.log(predicted_parameter_vector).mean()\n",
    "        critic_loss = delta ** 2\n",
    "        print(f\"actor loss {actor_loss}\")\n",
    "        print(f\"critic loss {critic_loss}\")\n",
    "        # We're weighting critic loss by 0.5\n",
    "        critic_loss_weight = 0.5\n",
    "        loss = actor_loss + critic_loss_weight * critic_loss\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        \n",
    "        if step_idx % step_optimizer_every_n_episode_steps == 0:\n",
    "            if GRADIENT_CLIPPING is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(actor_model.parameters(), GRADIENT_CLIPPING)\n",
    "                torch.nn.utils.clip_grad_norm_(critic_model.parameters(), GRADIENT_CLIPPING)\n",
    "\n",
    "            actor_optimizer.step()\n",
    "            critic_optimizer.step()\n",
    "        \n",
    "        # Now that we've learned from this step, update current state variable\n",
    "        next_state[1] = next_state[1].detach()\n",
    "        next_state[2] = next_state[2].detach()\n",
    "        state = next_state\n",
    "        \n",
    "        all_rewards.append(reward.item())\n",
    "        # print(predicted_parameter_vector)\n",
    "        # print(ground_truth_params_vector)\n",
    "        with torch.no_grad():\n",
    "            params_mse = ((predicted_parameter_vector.to(device) - ground_truth_params_vector.to(device))**2).mean().item()\n",
    "        episode_step_pbar.set_postfix({\"loss\" : loss.item(), \"reward\" : reward.item(), \"params_mse\" : params_mse})\n",
    "        break\n",
    "    break\n",
    "    # print (f\"Episode {episode_idx + 1}: total reward: {sum(all_rewards)}, return: {compute_returns(all_rewards, gamma)[0]}\")\n",
    "    episode_pbar.set_postfix({\"total reward\" : sum(all_rewards), \"return\" : compute_returns(all_rewards, gamma)[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6aadfb841290bcfc72af95a2a07db7a853e47c0d88647a3676c8f524c44e33bf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('drl_project': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
