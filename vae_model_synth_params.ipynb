{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53bb60c6-06b8-4275-97a0-9d8b67686b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import vae_model.build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70073708-c719-4198-bfb8-ea7d1ee8e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dir = './vae_model'\n",
    "\n",
    "# Create model\n",
    "with open(os.path.join(config_dir, 'subset_samplers.pickle'), 'rb') as f:\n",
    "    idx_helper = pickle.load(f)\n",
    "with open(os.path.join(config_dir, 'config_train.pickle'), 'rb') as f:\n",
    "    conf_train = pickle.load(f)\n",
    "with open(os.path.join(config_dir, 'config_model.pickle'), 'rb') as f:\n",
    "    conf_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bb57765-30b6-40ac-87d9-a861467d9cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORCE BIGGER ENC/DEC NETWORKS = False\n",
      "MIDI NOTES = ((60, 100),)\n",
      "self.spectrogram_input_size (257, 347)\n"
     ]
    }
   ],
   "source": [
    "_, _, _, model = vae_model.build.build_extended_ae_model(conf_model, conf_train,\n",
    "                                                                 idx_helper)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cf093b9-a404-44f2-adb7-c7cbb7a1c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from checkpoint\n",
    "checkpoint_state_dict = torch.load(\n",
    "    os.path.join(config_dir, '00133.tar'), \n",
    "    map_location=torch.device('cpu')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c76e5064-6345-42af-9f27-46d5858ee972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint_state_dict['ae_model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "675ea602-9aa6-45dc-9a13-2ebbc13cd604",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model, reg_model = model.ae_model, model.reg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dee03595-d2bd-473d-8361-d03d9471c42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7efe217faac0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c291cc99-b6e7-42d7-9ef6-d3135b96246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would come from the Dataset\n",
    "sample_spectrogram_input = torch.randn(1, 1, 257, 347)\n",
    "\n",
    "preset_UID = 0\n",
    "ref_midi_pitch = 60\n",
    "ref_midi_velocity = 100\n",
    "\n",
    "sample_info = torch.tensor([preset_UID, ref_midi_pitch, ref_midi_velocity], dtype=torch.int32).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d7fdb26-5ec4-4d31-81c4-4a08caf00bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_out = ae_model(sample_spectrogram_input, sample_info)  # Spectral VAE - tuple output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fe517d3-33ec-42b3-b40e-c986d4c84ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ae_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef911d37-035e-4630-885e-324f4d321f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, z_K_sampled, _, _ = ae_out\n",
    "v_out = reg_model(z_K_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "456a954d-85fb-4705-b5c9-5f9518e70c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 610])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6950fb3-72f1-44ae-8975-637f3d481c0f",
   "metadata": {},
   "source": [
    "### This `v_out` tensor is what's being used to get the full dexed synth params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "addb9446-9fd0-46d7-8d69-9f31f4fcf3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't import sounddevice!!!!\n"
     ]
    }
   ],
   "source": [
    "from data.preset import DexedPresetsParams\n",
    "import vae_config\n",
    "import data.build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13b7ee4c-c4aa-4c73-a074-875cab13200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config, train_config = vae_config.model, vae_config.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2edc1348-c62a-428e-a250-f17c5bd642e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PresetDataset] Cannot open '/home/ubuntu/environment/project/audio-synth-rl/data/stats/DexedDataset_spectrogram_nfft1024hop0256mels0257.json' stats file.\n",
      "[PresetDataset] No pre-computed spectrogram stats can be found. No normalization will be performed\n",
      "[PresetIndexesHelper] 144 learnable VSTi parameters, learnable tensor representation size: 610\n"
     ]
    }
   ],
   "source": [
    "dataset = data.build.get_dataset(model_config, train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc788133-a7b1-43c6-a1e1-0da8b0aa2935",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_presets_instance = DexedPresetsParams(learnable_presets=v_out, dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d01429b-cec0-4861-8617-d027a17e1e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 155])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_presets_instance.get_full().shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
